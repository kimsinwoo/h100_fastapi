# vLLM gateway + optional embedded engine. Python 3.10+
# Install in addition to or instead of transformers-based stack for production LLM serving.

fastapi>=0.109.0,<0.115.0
uvicorn[standard]>=0.27.0,<0.32.0
pydantic>=2.6.0,<2.10.0
pydantic-settings>=2.2.0,<2.6.0
httpx>=0.27.0,<0.28.0

# vLLM: use version compatible with your CUDA (e.g. 0.6.x or 0.10+)
vllm>=0.6.0

# Triton: vLLM 0.7.x는 triton 3.x와 호환되지 않음 (default_cache_dir 제거됨).
# "cannot import name 'default_cache_dir'" 오류 시 아래 중 하나 적용:
#   (1) triton 2.x 고정: pip install 'triton>=2.0,<3.0'
#   (2) vLLM 업그레이드: pip install -U 'vllm>=0.15'
triton>=2.0,<3.0
