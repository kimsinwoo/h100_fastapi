"""
Production-grade Z-Image-Turbo img2img service
Optimized for quality, stability, and commercial deployment
"""

from __future__ import annotations

import asyncio
import io
import logging
import time
import warnings
from typing import Any

from app.core.config import get_settings
from app.models.image_prompt_expert import ImagePromptExpert

logger = logging.getLogger(__name__)

_pipeline: Any = None
_lock = asyncio.Lock()
_device: Any = None

# ===== Z-Image-Turbo ê¶Œì¥ê°’ =====
# guidance_scale=0 ì´ë©´ negative_promptëŠ” ë¬´ì‹œë¨(ê³µì‹ ë¬¸ì„œ). í”½ì…€ì•„íŠ¸ë§Œ 1.8ë¡œ ì˜¬ë ¤ ë„¤ê±°í‹°ë¸Œ ì ìš©.
DEFAULT_GUIDANCE_SCALE = 0.0
PIXEL_ART_GUIDANCE_SCALE = 1.8  # í”½ì…€ì•„íŠ¸: voxel/3D ë¸”ë¡ ì°¨ë‹¨í•˜ë ¤ë©´ 1 ì´ìƒ í•„ìš”
DEFAULT_NUM_INFERENCE_STEPS = 8
MODEL_RESOLUTION = 1024

# ìŠ¤íƒ€ì¼ë³„ strength: í”½ì…€ì•„íŠ¸ëŠ” ë‚®ì¶°ì•¼ 3D ë¸”ë¡/ë³µì…€ ë°©ì§€, ë‚˜ë¨¸ì§€ëŠ” ê° íŠ¹ì„± ìœ ì§€
STRENGTH_BY_STYLE: dict[str, tuple[float, float]] = {
    "pixel art": (0.36, 0.46),      # ë§¤ìš° ë‚®ê²Œ ìœ ì§€í•´ì•¼ ìˆœìˆ˜ 2D ìŠ¤í”„ë¼ì´íŠ¸ë§Œ ë‚˜ì˜´
    "anime": (0.48, 0.56),
    "realistic": (0.46, 0.56),
    "watercolor": (0.48, 0.56),
    "cyberpunk": (0.48, 0.56),
    "oil painting": (0.48, 0.56),
    "sketch": (0.48, 0.56),
    "cinematic": (0.46, 0.54),
    "fantasy art": (0.48, 0.56),
    "3d render": (0.50, 0.58),
}
DEFAULT_STRENGTH_FALLBACK = 0.50
STRENGTH_GLOBAL_MAX = 0.58

# í”½ì…€ ì•„íŠ¸ ì„ íƒ ì‹œ ë„¤ê±°í‹°ë¸Œì— ì¶”ê°€ë¡œ ë„£ì–´ 3D/ë³µì…€ ì™„ì „ ì°¨ë‹¨
PIXEL_ART_NEGATIVE_SUFFIX = (
    ", voxel art, 3D pixel art, blocky 3D, Minecraft style, lego style, "
    "sweater made of blocks, dog made of cubes, volumetric blocks, 2.5D"
)

# ìˆœìˆ˜ 2D í”½ì…€ ì•„íŠ¸ë§Œ (ë§ˆì¸í¬ë˜í”„íŠ¸/ë³µì…€/3D ë¸”ë¡ ì™„ì „ ë°°ì œ)
# ============================================================
# Device
# ============================================================

def _resolve_device():
    import torch
    if torch.cuda.is_available():
        return torch.device("cuda")
    if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


# ============================================================
# Load Pipeline
# ============================================================

def _load_pipeline_sync():
    global _pipeline, _device

    import torch
    from diffusers import ZImageImg2ImgPipeline

    settings = get_settings()
    _device = _resolve_device()

    dtype = torch.bfloat16 if (_device.type == "cuda" and getattr(torch, "bfloat16", None)) else torch.float32

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")

        pipe = ZImageImg2ImgPipeline.from_pretrained(
            settings.model_id,
            torch_dtype=dtype,
            low_cpu_mem_usage=True,
        )
        # Z-Image ì „ìš© ìŠ¤ì¼€ì¤„ëŸ¬ ìœ ì§€ (UniPCMultistepScheduler êµì²´ ì‹œ set_timesteps AssertionError ë°œìƒ)

        for method_name in ("enable_attention_slicing", "enable_vae_slicing", "enable_vae_tiling"):
            method = getattr(pipe, method_name, None)
            if callable(method):
                try:
                    method()
                except Exception:
                    pass

        pipe = pipe.to(_device)
        # VAEëŠ” íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼ dtype ìœ ì§€ (float32ë¡œ ë°”ê¾¸ë©´ Half/float ë¶ˆì¼ì¹˜ë¡œ ì˜¤ë¥˜)

    logger.info(
        "Pipeline loaded on %s (dtype=%s)",
        _device,
        dtype,
    )

    _pipeline = pipe
    return pipe


async def get_pipeline():
    global _pipeline
    async with _lock:
        if _pipeline is None:
            loop = asyncio.get_event_loop()
            _pipeline = await loop.run_in_executor(
                None, _load_pipeline_sync
            )
        return _pipeline


# ============================================================
# Inference
# ============================================================

def _resize_keep_ratio(in_w: int, in_h: int, max_side: int) -> tuple[int, int]:
    """ì…ë ¥ ë¹„ìœ¨ ìœ ì§€í•˜ë©° ê¸´ ë³€ì´ max_side ì´í•˜, 8ì˜ ë°°ìˆ˜ë¡œ (out_w, out_h) ê³„ì‚°."""
    if in_w <= 0 or in_h <= 0:
        return (max_side, max_side)
    scale = max_side / max(in_w, in_h)
    out_w = max(64, min(max_side, round(in_w * scale)))
    out_h = max(64, min(max_side, round(in_h * scale)))
    out_w = (out_w // 8) * 8
    out_h = (out_h // 8) * 8
    return (max(64, out_w), max(64, out_h))


def _run_inference_sync(
    image_bytes: bytes,
    prompt: str,
    negative_prompt: str,
    strength: float,
    num_steps: int,
    guidance_scale: float,
    width: int,
    height: int,
    seed: int | None,
) -> bytes:

    import torch
    from PIL import Image

    global _pipeline, _device

    if _pipeline is None:
        raise RuntimeError("Pipeline not loaded")

    img = Image.open(io.BytesIO(image_bytes)).convert("RGB")
    target_w, target_h = width, height
    if img.width != target_w or img.height != target_h:
        img = img.resize((target_w, target_h), Image.Resampling.LANCZOS)

    # Deterministic seed
    generator = torch.Generator(device=_device)
    if seed is not None:
        generator.manual_seed(seed)

    logger.info(
        "Running img2img | steps=%d | guidance=%.2f | strength=%.2f",
        num_steps,
        guidance_scale,
        strength,
    )

    with torch.inference_mode():
        result = _pipeline(
            prompt=prompt,
            negative_prompt=negative_prompt,
            image=img,
            strength=strength,
            num_inference_steps=num_steps,
            guidance_scale=guidance_scale,
            generator=generator,
            output_type="pil",   # ğŸ”¥ Let diffusers handle decoding
        )

    image = result.images[0]

    buf = io.BytesIO()
    image.save(buf, format="PNG")
    return buf.getvalue()


# ============================================================
# Public API
# ============================================================

async def run_image_to_image(
    image_bytes: bytes,
    style_key: str,
    custom_prompt: str | None = None,
    strength: float | None = None,
    num_steps: int | None = None,
    size: int | None = None,
    seed: int | None = None,
):

    pipe = await get_pipeline()
    if pipe is None:
        raise RuntimeError("Model not available")

    settings = get_settings()
    style_lower = style_key.lower().strip()

    # ImagePromptExpert + êµ¬ì„± ìœ ì§€ (ë³µì¡í•œ ì‚¬ì§„ë„ ë ˆì´ì•„ì›ƒ ìœ ì§€)
    compiled = ImagePromptExpert.compile(
        style_key, custom_prompt or "", aspect_ratio="1:1"
    )
    prompt = compiled["final_prompt"]
    prompt += ", preserve original composition, same layout and pose, keep subject arrangement"
    negative_prompt = compiled["negative_prompt"]
    if "pixel" in style_lower:
        negative_prompt = negative_prompt + PIXEL_ART_NEGATIVE_SUFFIX

    # ìŠ¤íƒ€ì¼ë³„ strength ìƒí•œÂ·ê¸°ë³¸ê°’
    default_st, max_st = STRENGTH_BY_STYLE.get(
        style_lower, (DEFAULT_STRENGTH_FALLBACK, STRENGTH_GLOBAL_MAX)
    )
    strength = strength if strength is not None else default_st
    strength = max(0.0, min(STRENGTH_GLOBAL_MAX, min(1.0, strength), max_st))

    num_steps = max(1, min(50, num_steps or DEFAULT_NUM_INFERENCE_STEPS))
    guidance_scale = PIXEL_ART_GUIDANCE_SCALE if "pixel" in style_lower else DEFAULT_GUIDANCE_SCALE

    max_side = size or MODEL_RESOLUTION
    from PIL import Image
    with Image.open(io.BytesIO(image_bytes)) as tmp:
        tmp.load()
        in_w, in_h = tmp.width, tmp.height
    target_w, target_h = _resize_keep_ratio(in_w, in_h, max_side)

    loop = asyncio.get_event_loop()
    start = time.perf_counter()

    result = await loop.run_in_executor(
        None,
        lambda: _run_inference_sync(
            image_bytes,
            prompt,
            negative_prompt,
            strength,
            num_steps,
            guidance_scale,
            target_w,
            target_h,
            seed,
        ),
    )

    elapsed = time.perf_counter() - start
    return result, elapsed


# ============================================================
# Utilities
# ============================================================

def is_pipeline_loaded() -> bool:
    return _pipeline is not None


def is_gpu_available() -> bool:
    try:
        import torch
        return torch.cuda.is_available()
    except Exception:
        return False
